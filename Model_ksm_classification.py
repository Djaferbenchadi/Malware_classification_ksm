# %%

import subprocess
import hdf5storage
import numpy as np
from sklearn.preprocessing import normalize

def run_script(sgm, subdim):
  cmd = ['python', 'script.py', '--sgm', str(sgm), '--subdim', str(subdim)]
  subprocess.run(cmd)



# List of parameter sets for Hyperparameter optimization
parameter_sets = [(0.1, i) for i in range(3, 81)]


train_x = []
test_x_ = []

# Load malware dataset (2D matrix) (PS: data were loaded as Mat file after applying L2 normalization) 
for i in range(1, 26):
    file_name = "X" + str(i) + ".mat"
    file_path = "MALIMG_multiple/" + file_name
    data = hdf5storage.loadmat(file_path)
    variable_name = "X"+str(i)
    data_transposed = np.array(data[variable_name].T)
    train_x.append(data_transposed)

for i in range(1, 26):
    file_name = "Y" + str(i) + ".mat"
    file_path = "MALIMG_multiple/" + file_name
    data = hdf5storage.loadmat(file_path)
    variable_name = "Y"+str(i)
    data_transposed = np.array(data[variable_name].T)
    test_x_.append(data_transposed)
    
train_y = np.arange(25)

test_x_all = np.concatenate(test_x_, axis=0)
test_x = [test_x_all[i, np.newaxis, :] for i in range(test_x_all.shape[0])]
test_y = np.concatenate([np.full(len(test_x_[i]), i) for i in range(25)])

#parameter-------------------------------------------------------------------------------
class_num = 25                               
class_info = np.array([97, 91, 2824, 1491, 173, 81, 175, 121, 152, 137, 306, 356, 153, 159, 98, 134, 111, 117, 133, 55, 103, 107, 383, 72, 775])   
#----------------------------------------------------------------------------------------


#normalization---------------------------------------------------------------------------
class_index = []
count_c = 0
for class_i in class_info:
    if count_c == 0:
        class_index.append(0)
        class_index.append(class_info[0])
    else:
        class_index.append(class_index[count_c] + class_info[count_c])
    count_c += 1
class_index = np.array(class_index)

count = 0
for train_data in train_x:
    train_x[count] = normalize(train_data, axis=1)
    count += 1
count = 0
for test_data in test_x:
    test_x[count] = normalize(test_data, axis=1)
    count += 1
#----------------------------------------------------------------------------------------


#------------------------for training------------------------

for sgm, subdim in parameter_sets:
  run_script(sgm, subdim)
  for var in list(globals().keys()):
    if var not in ['function variables','accuracy_score','gauss_class_mat_diff','gauss_gram_mat','gauss_gram_two','gauss_projection_diff','get_ipython','jit','parentpath1','rand','randint','run_script','trace_this_thread','train_test_split','special variables','__','___','__doc__','__file__','__loader__','__name__','__package__','__spec__','__vsc_ipynb_file__','__builtin__','__builtins__','debugpy','exit','hdf5storage','np','os','pd','quit','random','subprocess','sys','_VSCODE_hashlib','_VSCODE_types','_dh','_VSCODE_compute_hash','_VSCODE_wrapped_run_cell','normalize','sgm', 'subdim', 'train_x','train_y','test_x','test_y']:
      del globals()[var]

  import os
  from numpy.random import randint, rand
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score
  import random
  import matplotlib.pyplot as plt
  from numba import jit, void, f8, njit  
  from sklearn.metrics import confusion_matrix
  from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support
  import pandas as pd
  from numpy.random import randint, rand
  from numba import jit, void, f8, njit
  import seaborn as sns



  @jit(void(f8[:, :], f8[:, :]))
  def gauss_gram_mat(x, K):
    n_points = len(x)
    n_dim = len(x[0])
    b = 0
    for j in range(n_points):
      for i in range(n_points):
        for k in range(n_dim):
          b = (x[i][k] - x[j][k])
          K[i][j] += b * b

  @jit
  def gauss_gram_two(X, Y, K_t):
      for i in range(X.shape[0]):
          for j in range(Y.shape[0]):
              for k in range(X.shape[1]):
                  b = (X[i][k] - Y[j][k])
                  K_t[i][j] += b * b

  def l2_kernel(X, Y): ##L2-norms of x and y.
      XX = (X ** 2).sum(axis=0)[:, None]
      YY = (Y ** 2).sum(axis=0)[None, :]

      return XX - 2 * (X.T @ Y) + YY

  def rbf_kernel(X, Y, sigma=None):
      n_dims = X.shape[0]
      if sigma is None:
          sigma = np.sqrt(n_dims / 2)

      x = l2_kernel(X, Y)

      return np.exp(-0.5 * x / (sigma ** 2))
   
  def dual_vectors(K, n_subdims=None, higher=True, elim=False, eps=1e-6):
      e, A = np.linalg.eigh(K)
      e[(e < eps)] = eps

      A = A / np.sqrt(e)

      if elim:
          e = e[(e > eps)]
          A = A[:, (e > eps)]

      if higher:
          return A[:, -n_subdims:]

      return A[:, :n_subdims]

  print("sigma:",sgm,"subdim:",subdim)   

 

  K_class = []

  for train_data in train_x:
      K1 = np.zeros((train_data.shape[0], train_data.shape[0]))
      gauss_gram_mat(train_data, K1)
      K1 = np.exp(- K1 / (2 * sgm))
      X1_coeffs = dual_vectors(K1, n_subdims=subdim)
      K_class.append(X1_coeffs)



  #------------------------for testing------------------------
  # Kernel projection for testing data 
  test_np = np.array(test_x)
  test_np = test_np.reshape([test_np.shape[0], test_np.shape[2]])


  K_input = []
  for train_data in train_x:
      K_1 = np.zeros((test_np.shape[0], train_data.shape[0]))
      gauss_gram_two(test_np, train_data, K_1)
      K_1 = np.exp(- K_1 / (2 * sgm))
      K_1 = K_1.T
      K_input.append(K_1)

  #--------------------Calculate projection length--------------------
  similarity_all = []
  similarity_all_1 = []

  for j in range(25):
      similarity = []
      for i in range(K_input[j].shape[1]):
          data_input = K_input[j][:, i]
          length = np.linalg.norm(K_class[j].T @ data_input, ord=2)
          similarity.append(length)
      similarity_all.append(similarity)


  # Assign labels to classes based on highest similarity
  for k in range(935):
      max_similarity = -1
      max_label = -1
      for i in range(25):
          if similarity_all[i][k] > max_similarity:
              max_similarity = similarity_all[i][k]
              max_label = i
      similarity_all_1.append(max_label)



  precision, recall, f1_score, _ = precision_recall_fscore_support(test_y, similarity_all_1, average='macro')
  print('Precision: {:.3f}'.format(precision))
  print('Recall: {:.3f}'.format(recall))
  print('F1 score: {:.3f}'.format(f1_score))


  Accuracy = accuracy_score(similarity_all_1, test_y)
  print("accuracy:", Accuracy)

  conf_mat = confusion_matrix(test_y, similarity_all_1)
  plt.figure(figsize=(10,8))
  sns.heatmap(conf_mat,annot=True,fmt='d',cmap='Blues',xticklabels=['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g', 'C2LOP.P', 'Dialplatform.B','Dontovo.A','Fakerean','Instantaccess' ,'Lolyda.AA1','Lolyda.AA2','Lolyda.AA3','Lolyda.AT','Malex.gen!J','Obfuscator.AD','Rbot!gen','Skintrim.N','Swizzor.gen!E','Swizzor.gen!I','VB.AT','Wintrim.BX','Yuner.A'],yticklabels=['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g', 'C2LOP.P', 'Dialplatform.B','Dontovo.A','Fakerean','Instantaccess' ,'Lolyda.AA1','Lolyda.AA2','Lolyda.AA3','Lolyda.AT','Malex.gen!J','Obfuscator.AD','Rbot!gen','Skintrim.N','Swizzor.gen!E','Swizzor.gen!I','VB.AT','Wintrim.BX','Yuner.A'])
  plt.xlabel('Predicted labels')
  plt.ylabel('True labels')
  plt.show()




